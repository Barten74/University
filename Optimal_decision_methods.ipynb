{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y = 1000 * K^(1/3) * L^(1/2) * R^(1/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B = pY - 0.1K - 0.05L - 0.01R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подставим Y в B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B = p(1000 * K^(1/3) * L^(1/3) * R^(1/6) - 0.1K - 0.05L - 0.01R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p = 100 L = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B = 100000 * K^(1/3) * 80^(1/2) * R^(1/6) - 0.1K - 4 - 0.01R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(x) = B, x1 = K, x2 = R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(x) = 100000 * x1^(1/3) * 80^(1/2) * x2^(1/6) - 0.1x1 - 4 - 0.01x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x1>=0, x2>=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(x) -> max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим последовательность функций со штрафами:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pk = f(x) - Rk(x1^2 + x2^2),          где Rk = {1/2^k}, и k = 1,2,3,4 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F =       32932085.998190545\n",
      "F_true =  2279967928902.262\n",
      "7  iterations in gradient descent\n",
      "------------0-----------------\n",
      "F =       41491733.511960216\n",
      "F_true =  2279967928902.262\n",
      "5  iterations in gradient descent\n",
      "------------1-----------------\n",
      "F =       52276157.552268945\n",
      "F_true =  2279967928902.262\n",
      "67  iterations in gradient descent\n",
      "------------2-----------------\n",
      "F =       65863592.37786081\n",
      "F_true =  2279967928902.262\n",
      "68  iterations in gradient descent\n",
      "------------3-----------------\n",
      "F =       82982547.13194412\n",
      "F_true =  2279967928902.262\n",
      "69  iterations in gradient descent\n",
      "------------4-----------------\n",
      "F =       104550855.7149011\n",
      "F_true =  2279967928902.262\n",
      "70  iterations in gradient descent\n",
      "------------5-----------------\n",
      "F =       131724867.93899655\n",
      "F_true =  2279967928902.262\n",
      "71  iterations in gradient descent\n",
      "------------6-----------------\n",
      "F =       165961416.3831051\n",
      "F_true =  2279967928902.262\n",
      "72  iterations in gradient descent\n",
      "------------7-----------------\n",
      "F =       209095873.02710313\n",
      "F_true =  2279967928902.262\n",
      "73  iterations in gradient descent\n",
      "------------8-----------------\n",
      "F =       263440467.92440617\n",
      "F_true =  2279967928902.262\n",
      "74  iterations in gradient descent\n",
      "------------9-----------------\n",
      "F =       331908120.8663894\n",
      "F_true =  2279967928902.262\n",
      "75  iterations in gradient descent\n",
      "------------10-----------------\n",
      "F =       418168392.68004924\n",
      "F_true =  2279967928902.262\n",
      "76  iterations in gradient descent\n",
      "------------11-----------------\n",
      "F =       526843865.4912571\n",
      "F_true =  2279967928902.262\n",
      "77  iterations in gradient descent\n",
      "------------12-----------------\n",
      "F =       663757398.0242468\n",
      "F_true =  2279967928902.262\n",
      "78  iterations in gradient descent\n",
      "------------13-----------------\n",
      "F =       836243380.6159246\n",
      "F_true =  2279967928902.262\n",
      "79  iterations in gradient descent\n",
      "------------14-----------------\n",
      "F =       1053539468.0219637\n",
      "F_true =  2279967928902.262\n",
      "80  iterations in gradient descent\n",
      "------------15-----------------\n",
      "F =       1327279459.1120524\n",
      "F_true =  2279967928902.262\n",
      "81  iterations in gradient descent\n",
      "------------16-----------------\n",
      "F =       1672113219.0015056\n",
      "F_true =  2279967928902.262\n",
      "82  iterations in gradient descent\n",
      "------------17-----------------\n",
      "F =       2106486038.6353385\n",
      "F_true =  2279967928902.262\n",
      "83  iterations in gradient descent\n",
      "------------18-----------------\n",
      "F =       2653617880.269141\n",
      "F_true =  2279967928902.262\n",
      "84  iterations in gradient descent\n",
      "------------19-----------------\n",
      "F =       3342732889.4717073\n",
      "F_true =  2279967928902.262\n",
      "84  iterations in gradient descent\n",
      "------------20-----------------\n",
      "F =       4210601726.181314\n",
      "F_true =  2279967928902.262\n",
      "637  iterations in gradient descent\n",
      "------------21-----------------\n",
      "F =       5303474089.585661\n",
      "F_true =  2279967928902.262\n",
      "645  iterations in gradient descent\n",
      "------------22-----------------\n",
      "F =       6679496550.318197\n",
      "F_true =  2279967928902.262\n",
      "653  iterations in gradient descent\n",
      "------------23-----------------\n",
      "F =       8411731999.27612\n",
      "F_true =  2279967928902.262\n",
      "661  iterations in gradient descent\n",
      "------------24-----------------\n",
      "F =       10591921377.685198\n",
      "F_true =  2279967928902.262\n",
      "669  iterations in gradient descent\n",
      "------------25-----------------\n",
      "F =       13335155921.734083\n",
      "F_true =  2279967928902.262\n",
      "677  iterations in gradient descent\n",
      "------------26-----------------\n",
      "F =       16785657584.178658\n",
      "F_true =  2279967928902.262\n",
      "685  iterations in gradient descent\n",
      "------------27-----------------\n",
      "F =       21123894270.365257\n",
      "F_true =  2279967928902.262\n",
      "693  iterations in gradient descent\n",
      "------------28-----------------\n",
      "F =       26575280365.148464\n",
      "F_true =  2279967928902.262\n",
      "701  iterations in gradient descent\n",
      "------------29-----------------\n",
      "F =       33420723453.825356\n",
      "F_true =  2279967928902.262\n",
      "709  iterations in gradient descent\n",
      "------------30-----------------\n",
      "F =       42009261129.54817\n",
      "F_true =  2279967928902.262\n",
      "716  iterations in gradient descent\n",
      "------------31-----------------\n",
      "F =       52772964433.95623\n",
      "F_true =  2279967928902.262\n",
      "723  iterations in gradient descent\n",
      "------------32-----------------\n",
      "F =       66244130579.829834\n",
      "F_true =  2279967928902.262\n",
      "730  iterations in gradient descent\n",
      "------------33-----------------\n",
      "F =       83074491671.92229\n",
      "F_true =  2279967928902.262\n",
      "736  iterations in gradient descent\n",
      "------------34-----------------\n",
      "F =       104055646159.57614\n",
      "F_true =  2279967928902.262\n",
      "742  iterations in gradient descent\n",
      "------------35-----------------\n",
      "F =       130139060030.65192\n",
      "F_true =  2279967928902.262\n",
      "747  iterations in gradient descent\n",
      "------------36-----------------\n",
      "F =       162452632875.14038\n",
      "F_true =  2279967928902.262\n",
      "752  iterations in gradient descent\n",
      "------------37-----------------\n",
      "F =       202308801478.8483\n",
      "F_true =  2279967928902.262\n",
      "755  iterations in gradient descent\n",
      "------------38-----------------\n",
      "F =       251196301126.7056\n",
      "F_true =  2279967928902.262\n",
      "757  iterations in gradient descent\n",
      "------------39-----------------\n",
      "F =       310744000720.8133\n",
      "F_true =  2279967928902.262\n",
      "758  iterations in gradient descent\n",
      "------------40-----------------\n",
      "F =       382641049944.21765\n",
      "F_true =  2279967928902.262\n",
      "756  iterations in gradient descent\n",
      "------------41-----------------\n",
      "F =       468494205250.5145\n",
      "F_true =  2279967928902.262\n",
      "2822  iterations in gradient descent\n",
      "------------42-----------------\n",
      "F =       569603649215.6072\n",
      "F_true =  2279967928902.262\n",
      "1445  iterations in gradient descent\n",
      "------------43-----------------\n",
      "F =       686648661588.1196\n",
      "F_true =  2279967928902.262\n",
      "5334  iterations in gradient descent\n",
      "------------44-----------------\n",
      "F =       819302998311.1245\n",
      "F_true =  2279967928902.262\n",
      "2711  iterations in gradient descent\n",
      "------------45-----------------\n",
      "F =       965855329488.2576\n",
      "F_true =  2279967928902.262\n",
      "9898  iterations in gradient descent\n",
      "------------46-----------------\n",
      "F =       1122987360743.0293\n",
      "F_true =  2279967928902.262\n",
      "4990  iterations in gradient descent\n",
      "------------47-----------------\n",
      "F =       1285913542441.6116\n",
      "F_true =  2279967928902.262\n",
      "18009  iterations in gradient descent\n",
      "------------48-----------------\n",
      "F =       1449000217579.7207\n",
      "F_true =  2279967928902.262\n",
      "8982  iterations in gradient descent\n",
      "------------49-----------------\n",
      "F =       1606668637182.8145\n",
      "F_true =  2279967928902.262\n",
      "31779  iterations in gradient descent\n",
      "------------50-----------------\n",
      "F =       1754040691995.0159\n",
      "F_true =  2279967928902.262\n",
      "15429  iterations in gradient descent\n",
      "------------51-----------------\n",
      "F =       1886935848977.8665\n",
      "F_true =  2279967928902.262\n",
      "52109  iterations in gradient descent\n",
      "------------52-----------------\n",
      "F =       2001563348066.8572\n",
      "F_true =  2279967928902.262\n",
      "23694  iterations in gradient descent\n",
      "------------53-----------------\n",
      "F =       2094714701488.5984\n",
      "F_true =  2279967928902.262\n",
      "72129  iterations in gradient descent\n",
      "------------54-----------------\n",
      "F =       2164781564278.633\n",
      "F_true =  2279967928902.262\n",
      "28537  iterations in gradient descent\n",
      "------------55-----------------\n",
      "F =       2212907757948.0522\n",
      "F_true =  2279967928902.262\n",
      "72064  iterations in gradient descent\n",
      "------------56-----------------\n",
      "F =       2243031403816.155\n",
      "F_true =  2279967928902.262\n",
      "23279  iterations in gradient descent\n",
      "------------57-----------------\n",
      "F =       2260425039980.277\n",
      "F_true =  2279967928902.262\n",
      "47804  iterations in gradient descent\n",
      "------------58-----------------\n",
      "F =       2269888581982.9365\n",
      "F_true =  2279967928902.262\n",
      "13212  iterations in gradient descent\n",
      "------------59-----------------\n",
      "F =       2274845291352.7637\n",
      "F_true =  2279967928902.262\n",
      "23920  iterations in gradient descent\n",
      "------------60-----------------\n",
      "F =       2277385034274.0586\n",
      "F_true =  2279967928902.262\n",
      "6192  iterations in gradient descent\n",
      "------------61-----------------\n",
      "F =       2278670976998.192\n",
      "F_true =  2279967928902.262\n",
      "10535  iterations in gradient descent\n",
      "------------62-----------------\n",
      "F =       2279318062616.016\n",
      "F_true =  2279967928902.262\n",
      "2692  iterations in gradient descent\n",
      "------------63-----------------\n",
      "F =       2279642646320.3438\n",
      "F_true =  2279967928902.262\n",
      "4367  iterations in gradient descent\n",
      "------------64-----------------\n",
      "F =       2279805200045.7\n",
      "F_true =  2279967928902.262\n",
      "1121  iterations in gradient descent\n",
      "------------65-----------------\n",
      "F =       2279886542527.121\n",
      "F_true =  2279967928902.262\n",
      "1714  iterations in gradient descent\n",
      "------------66-----------------\n",
      "F =       2279927230235.1133\n",
      "F_true =  2279967928902.262\n",
      "451  iterations in gradient descent\n",
      "------------67-----------------\n",
      "F =       2279947578183.512\n",
      "F_true =  2279967928902.262\n",
      "631  iterations in gradient descent\n",
      "------------68-----------------\n",
      "F =       2279957753197.2744\n",
      "F_true =  2279967928902.262\n",
      "294  iterations in gradient descent\n",
      "------------69-----------------\n",
      "F =       2279962840950.3003\n",
      "F_true =  2279967928902.262\n",
      "329  iterations in gradient descent\n",
      "------------70-----------------\n",
      "F =       2279965384902.2217\n",
      "F_true =  2279967928902.262\n",
      "168  iterations in gradient descent\n",
      "------------71-----------------\n",
      "F =       2279966656883.1733\n",
      "F_true =  2279967928902.262\n",
      "93  iterations in gradient descent\n",
      "------------72-----------------\n",
      "F =       2279967292870.1675\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------73-----------------\n",
      "F =       2279967610863.661\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------74-----------------\n",
      "F =       2279967769860.406\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------75-----------------\n",
      "F =       2279967849358.7744\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------76-----------------\n",
      "F =       2279967889107.9614\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------77-----------------\n",
      "F =       2279967908982.5527\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------78-----------------\n",
      "F =       2279967918919.8496\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------79-----------------\n",
      "F =       2279967923888.4976\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------80-----------------\n",
      "F =       2279967926372.8223\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------81-----------------\n",
      "F =       2279967927614.985\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------82-----------------\n",
      "F =       2279967928236.0654\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------83-----------------\n",
      "F =       2279967928546.607\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------84-----------------\n",
      "F =       2279967928701.8755\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------85-----------------\n",
      "F =       2279967928779.5107\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------86-----------------\n",
      "F =       2279967928818.3276\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------87-----------------\n",
      "F =       2279967928837.737\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------88-----------------\n",
      "F =       2279967928847.4414\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------89-----------------\n",
      "F =       2279967928852.2935\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------90-----------------\n",
      "F =       2279967928854.719\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------91-----------------\n",
      "F =       2279967928855.932\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------92-----------------\n",
      "F =       2279967928856.539\n",
      "F_true =  2279967928902.262\n",
      "1  iterations in gradient descent\n",
      "------------93-----------------\n",
      "STOP\n",
      "F_last =   2279967928855.932\n",
      "F_new =  2279967928856.539\n",
      "F_True =    2279967928902.262\n"
     ]
    }
   ],
   "source": [
    "A = np.array([10000,5000]) # Выбираем любую начальную точку из доп\n",
    "x1 = A[0]\n",
    "x2 = A[1]\n",
    "\n",
    "eps = 0.1 # выбираем критерий остановы для градиентного метода с дроблением шага\n",
    "lamb = 0.3 # кэффициент для для градиентного метода с дроблением шага\n",
    "F1 = 0\n",
    "F2 = 0\n",
    "\n",
    "iter_num_last = 0\n",
    "R = 1\n",
    "k = 0 # итерации для метода штрафных функций\n",
    "while True:\n",
    "    R = 1/(2**k) # Определяем R как 1/(2^k)  \n",
    "    # Определяем целевую функцию со штрафом, чтобы сравнивать значения на каждой итерации\n",
    "    F_new = (10**5)*x1**(1/3)*(80**(1/2))*x2**(1/6) - 0.1*x1 - 0.01*x2 - R*(x1**2) - R*(x2**2)\n",
    "    F_last = 0\n",
    "    iter_num_current = 0\n",
    "    \n",
    "#########################################################################    \n",
    "# Алгоритм поиска максимума методом градиентного спуска с дроблением шага\n",
    "\n",
    "    koef = 0.1 # кэффициент, используемый в критерии(нужно ли дробить шаг)для для градиентного метода с дроеблением шага\n",
    "    while abs(F_new - f_last) > eps: # Критерий останова\n",
    "        iter_num_current += 1 # подсчёт итераций поиска максимума\n",
    "        dFx1 = (298142*x1**(-2/3)*x2**(1/6) - 0.1 - 2*R*x1) # Производная целевой функции по x1\n",
    "        dFx2 = (149071*x1**(1/3)*x2**(-5/6) - 0.01 - 2*R*x2) # Производная целевой функции по x2\n",
    "        # Двигаемся к точке максимуа в направлении градиента, размер шага зависит от lamb\n",
    "        x1 = x1 + lamb*dFx1 \n",
    "        x2 = x2 + lamb*dFx2\n",
    "        f_last = F_new # Сохраняем предыдудщее значение целевой функции для сравнения с новой, чтобы определить критерий остановы для метода градиентного спуска\n",
    "        F_new = (10**5)*x1**(1/3)*(80**(1/2))*x2**(1/6) - 0.1*x1 - 0.01*x2 - R*(x1**2) - R*(x2**2)\n",
    "        # Критерий метода градиентного спуска с дроблением шага,\n",
    "        # который говорит о том, стоит ли дробить шаг:\n",
    "        if not F_new >= f_last - koef*lamb*(dFx1**2+dFx2**2):\n",
    "            lamb = lamb*0.95\n",
    "            \n",
    "    print('F =      ', F_new) # Вывод значения функции со штрафом\n",
    "    print('F_true = ', (4000000000000*(5**(1/3)))/3) # Вывод точного ответа задачи с Wolfram\n",
    "    print(iter_num_current, ' iterations in gradient descent')\n",
    "    \n",
    "    # Здесь осуществляется подбор новой Лямбды для следующей штрафной функции, это делается для ускорения сходимости.\n",
    "    if iter_num_last - iter_num_current >= 0:  \n",
    "        if iter_num_current < 100:\n",
    "            lamb = lamb*0.25\n",
    "        else:\n",
    "            lamb = lamb*0.5\n",
    "    else:\n",
    "        if iter_num_current > 1000:\n",
    "            lamb = lamb*4\n",
    "        else:\n",
    "            lamb = lamb*2\n",
    "    iter_num_last = iter_num_current\n",
    "##########################################################################\n",
    "    print('------------%d-----------------' % (k)) # вывод k - номер итерации метода штрафных функций\n",
    "    F2 = F1 # Cохраняем предыдущее значение для того чтобы сравнить с новым\n",
    "    F1 = F_new\n",
    "    k = k+1\n",
    "    if abs(F1-F2) < 1: #Критерий остановы для метода штрафных функций\n",
    "        print('STOP')\n",
    "        print('F_last =  ', F2)\n",
    "        print('F_new = ', F1)\n",
    "        print('F_True =   ', (4000000000000*(5**(1/3)))/3 ) # Точное реешение взято из WolframAlpha\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
